#参数估计、非参数估计实验报告

信息安全 李涵 1711290 

### 一、问题描述

​	现有一些数据，有四个特征值和一个标签，标签分为1、2、3共三类，现需采取一定的算法（最大似然估计、高斯核函数估计、KNN算法），根据特征值，给出分类标签结果。



### 二、解决方法

​	1.解决思路

​	假设特征符合高斯分布，利用最大似然估计确定均值和协方差矩阵，再据此和特征值计算相关的概率或概率密度，来判断分类结果；

​	使用高斯核函数估计，得到核函数的相关参数，然后据此和特征值计算相关的概率或概率密度，来判断分类结果；

​	依据KNN算法，计算距离测试样本特征欧式距离最小的k个训练样本，计算这k个最邻近样本的分类标签出现的频率，即为这些分类标签的概率，概率最大的分类标签即为测试样本的标签。

​	2.基本理论

​	对已知观测数据的概率分布符合某些模型（常用正态分布）的情况下，我们可以利用**参数估计**的方法来确定均值、方差等参数值，然后得出概率密度模型。常见的参数估计方法有**矩估计**和**极大似然估计**。

​	**矩估计**：样本与总体的原点矩是近似的，对于连续型随机变量：

期望：
$$
E(x)=\int_ {-\infty} ^ {+\infty} xf(x){\rm d}x
$$

方差：
$$
D(x)=\int_ {-\infty} ^ {+\infty} (x-E(x))^2 f(x){\rm d}x
$$
对于给定的样本X1,X2,……,Xn （X1,X2,……,Xn都是已知的），期望：
$$
E(x)=\bar{X}=\frac{\sum_{i=1}^n X_i}{n}
$$
方差：
$$
D(x)=\frac{\sum _{i=1}^n (X_i - \bar{X})^2 }{n}
$$
对于各随机变量都有
$$
E(X^2)=E(X)^2+D(X)
$$
​	根据给出的概率分布函数，计算总体的原点矩； 根据给定的样本，按照计算样本的原点矩；让总体的原点矩与样本的原点矩相等，解出参数。所得结果即为参数的矩估计值。



​	**极大似然估计**：似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。已知样本数据X1,X2,……,Xn （它们是独立，同分布）。它们发生的概率即为
$$
\prod _{i=1} ^{n} f(X_i)
$$
，所以得到一个关于参数的函数，即似然函数。要使得样本数据X1,X2,……,Xn同时发生的概率（即似然函数）取最大值，就可以估算参数。

根据似然函数：
$$
l(\theta)=p(D|\theta)=p(x_1,x_2,…,x_N|\theta)=\prod _{i=1} ^{N}  p(x_i|\theta)
$$
求解使得出现该组样本的概率最大的θ值：
$$
\hat{\theta}=arg \ max l(\theta)=arg \ max \prod _{i=1} ^{N}p(x_i|\theta)
$$
便于分析，定义
$$
H(\theta)=ln\ l(\theta)
$$
由于ln(x)函数是单调递增的，l(θ)和H(θ)的极大值点是相同的，而且ln(x)函数可以将积化为和，便于求导得到极大值点，简化计算，所以可以做如下转化
$$
\hat{\theta}=arg \ max \ H(\theta)=arg \ max \ lnl(\theta)=arg \ max \prod _{i=1} ^{N}ln\ p(x_i|\theta)
$$
似然函数对θ求导，令导数为0，求θ值，即
$$
{\triangledown }_\theta H(\theta)={\triangledown }_\theta ln\ l(\theta)=\sum _{i=1} ^{N} ln P（x_i|\theta)=0
$$
​	**非参数估计**，也称之为无参密度估计，它是一种对先验知识要求最少，完全依靠训练数据进行估计，而且可以用于任意形状密度估计的方法。常见的非参数估计方法有：直方图，核密度估计，K近邻估计

​	**直方图**：把数据的值域分为若干相等的区间，将数据按照值域区间分为若干组，每组形成一个矩形，该组数据的越多，矩形越高，将这些矩形依次排列组成的图形就是直方图。它提供给数据一个直观的形象，但只适合低维数据的情况，当维数较高时，直方图所需的空间将随着维数的增加呈指数级增加。

​	**核密度估计**：原理和直方图类似，是一种平滑的无参密度估计方法。对于一组数据，把数据的值域分为若干相等的区间，数据也按区间分为若干组，每组数据的个数和总参数个数的比率就是每个区间的概率值。相对于直方图法，它多了一个用于平滑数据的核函数。核密度估计方法适用于中小规模的数据集，可以很快地产生一个渐进无偏的密度估计，有良好的概率统计性质。具体来说，对于数据为X1,X2,……,Xn的数据集，在任意点x的核密度估计为:
$$
\hat f(x)=\frac{1}{nh} \sum _{i=1} ^{n} K(\frac{x-x_i}{h})
$$
K(x)称为核函数(Kernnal function)，通常满足对称性并且
$$
\int K(x){\rm d}x=1
$$
​	核函数是一种权函数，该估计利用数据点xi到**x**的距离来决定xi在估计点**x**的密度时所起的作用，距离x越近的样本点所起的作用就越大，其权值也就越大。式子中的h表示带宽，h越大，估计的密度函数就越平滑，但偏差可能会较大。如果h选的较小，那么估计的密度曲线和样本拟合的较好，但可能很不光滑，一般以均方误差最小为选择原则。常用的核函数有：高斯、余弦、均匀、三角等形式。
 	

​	**K近邻估计**：核密度估计的加权是以数据点到x的欧式距离为基准来进行的，而K近邻估计是无论欧式距离多少，只要是离x点的最近的k个点的其中之一就可以加权。即，K近邻密度估计可以表示为：
$$
\hat f(x)=\frac{k-1}{2n\ d_k(x)}
$$
d(x)表示点x到所有样本点的欧式距离，k的取值决定了估计密度的光滑程度，k越大越光滑。



​	3.算法流程

​	**极大似然估计**：

​	（1）分别计算三个类别的均值和协方差矩阵

​	（2）根据特征向量X和均值、协方差矩阵和高斯分布的概率密度函数，计算三个类别的概率（or概率密度）

​	（3）标签概率（or概率密度）最大的类别，即为测试样本的分类标签

​	**高斯核密度估计**：

​	（1）对训练集进行拟合，求取相关参数，得到高斯核函数（调用sklearn当中的kde相关函数）

​	（2）对测试集的数据进行评分（调用score_samples）

​	（3）分数最高的类别即为测试样本的分类标签

​	**k近邻估计**：

​	（1）对于每个给定测试对象，计算它与训练集中的每个对象的欧式距离

​	（2）找到距离最近的k个训练对象，作为测试对象的近邻

​	（3）返回前K个点中出现频率最高的类别作为测试数据的预测分类

​	

### 三、实验分析

​	1.实验数据

​	HWData3.csv 数据集包含 150 个样例，共分为 3 类，每类 50 个样例；文件每一行表示一个 样例，每个样例的前 4 列表示特征，第五列表示标签。 在极大似然估计当中，我们假设前四列的数据分别服从正态分布。

​	2.实验设计

​	导入相应的库，读取并数据进行预处理，设置全局变量等

```python
import numpy as np
import pandas as pd
import os
import copy
import math
from scipy import integrate 

basepath=os.path.abspath(os.path.dirname(__file__))
path=basepath+'/HWData3.txt'

a=np.loadtxt(path)
A=np.mat(a)

row=A.shape[0]
col=A.shape[1]
esp=0.000001
correct=0

```

​	利用np.mean()和np.cov()计算均值和协方差矩阵

```python
def cal_para(x):
    mean=np.zeros((1,col-1))
    for i in range(col-1):
        mean[0,i]=np.mean(x[:,i])
    
    cov=np.cov(x.T)
    return mean,cov
```

​	利用n维正态分布的概率密度公式，根据已求出的均值和协方差矩阵求概率密度

```python
def cal_density(x1,x2,x3,x4,mean,cov,n=4):
    #求概率密度
    x=np.zeros((1,col-1))
    x[0,0]=x1
    x[0,1]=x2
    x[0,2]=x3
    x[0,3]=x4
    det_cov=np.linalg.det(cov)#行列式
    cov_=np.linalg.inv(cov)#逆
    para=1/(pow((2*np.pi),n/2)*pow(det_cov,0.5))
    exponent=-0.5*(x-mean).dot(cov_).dot((x-mean).T)

    return para*pow(np.e,(exponent[0,0]))
```

​	根据概率密度，通过nquad函数求取特征值正负一个方差的区间内的积分，即概率（或者直接计算特征值那一个点的概率密度，被注释的那句，后测试发现直接用概率密度性能更好）

```python
def cal_probability(mean,x,cov,n=4):
    #根据概率密度求积分的概率
    #积分下限
    a1=x[0,0]-cov[0,0]
    a2=x[0,1]-cov[1,1]
    a3=x[0,2]-cov[2,2]
    a4=x[0,3]-cov[3,3]
    #积分下限
    b1=x[0,0]+cov[0,0]
    b2=x[0,1]+cov[1,1]
    b3=x[0,2]+cov[2,2]
    b4=x[0,3]+cov[3,3]
    
    #return cal_density(x[0,0],x[0,1],x[0,2],x[0,3],mean,cov)
    
    probability=integrate.nquad(cal_density,[[a1,b1],[a2,b2],[a3,b3],[a4,b4]],args=(mean,cov))
    return probability[0]    
```

​	进行五折交叉检验，将预测结果和测试样本原本的标签进行比较，求出分类性能

```python
for t in range(5):
    #五折交叉检验

    X1=A[0:50,0:4].copy()
    X2=A[50:100,0:4].copy()
    X3=A[100:150,0:4].copy()

    X1=np.delete(X1,[10*t,10*t+1,10*t+2,10*t+3,10*t+4,10*t+5,10*t+6,10*t+7,10*t+8,10*t+9], 0)
    X2=np.delete(X2,[10*t,10*t+1,10*t+2,10*t+3,10*t+4,10*t+5,10*t+6,10*t+7,10*t+8,10*t+9], 0)
    X3=np.delete(X3,[10*t,10*t+1,10*t+2,10*t+3,10*t+4,10*t+5,10*t+6,10*t+7,10*t+8,10*t+9], 0)
   
    test1=A[t*10:t*10+10,0:4].copy()
    test2=A[50+t*10:60+t*10,0:4].copy()
    test3=A[100+t*10:110+t*10,0:4].copy()

    mean1,cov1=cal_para(X1)
    mean2,cov2=cal_para(X2)
    mean3,cov3=cal_para(X3)

    #测试
    for test in test1:
        p1=cal_probability(mean1,test,cov1)
        p2=cal_probability(mean2,test,cov2)
        p3=cal_probability(mean3,test,cov3)
        print(p1,p2,p3)
        if(p1>p2)and(p1>p3):
            correct=correct+1
        
    for test in test2:
        p1=cal_probability(mean1,test,cov1)
        p2=cal_probability(mean2,test,cov2)
        p3=cal_probability(mean3,test,cov3)
        print(p1,p2,p3)
        if(p2>p1)and(p2>p3):
            correct=correct+1
        
    for test in test3:
        p1=cal_probability(mean1,test,cov1)
        p2=cal_probability(mean2,test,cov2)
        p3=cal_probability(mean3,test,cov3)
        print(p1,p2,p3)
        if(p3>p2)and(p3>p1):
            correct=correct+1
        
print(correct/150)
```

​	用高斯核函数拟合训练集（kde.KernelDensity(kernel="gaussian",bandwidth=h).fit(X)函数），再判定对于测试机的分数，分数高的类别即为分类结果，用五折交叉检验得出分类性能

```python
for t in range(5):
    #五折交叉检验

    X1=A[0:50,0:4].copy()
    X2=A[50:100,0:4].copy()
    X3=A[100:150,0:4].copy()

    X1=np.delete(X1,[10*t,10*t+1,10*t+2,10*t+3,10*t+4,10*t+5,10*t+6,10*t+7,10*t+8,10*t+9], 0)
    X2=np.delete(X2,[10*t,10*t+1,10*t+2,10*t+3,10*t+4,10*t+5,10*t+6,10*t+7,10*t+8,10*t+9], 0)
    X3=np.delete(X3,[10*t,10*t+1,10*t+2,10*t+3,10*t+4,10*t+5,10*t+6,10*t+7,10*t+8,10*t+9], 0)

    test1=A[t*10:t*10+10,0:4].copy()
    test2=A[50+t*10:60+t*10,0:4].copy()
    test3=A[100+t*10:110+t*10,0:4].copy()
    
    pattern1=kde.KernelDensity(kernel="gaussian",bandwidth=h).fit(X1)
    pattern2=kde.KernelDensity(kernel="gaussian",bandwidth=h).fit(X2)
    pattern3=kde.KernelDensity(kernel="gaussian",bandwidth=h).fit(X3)

    log_dens11=pattern1.score_samples(test1)
    log_dens12=pattern2.score_samples(test1)
    log_dens13=pattern3.score_samples(test1)
    
    log_dens21=pattern1.score_samples(test2)
    log_dens22=pattern2.score_samples(test2)
    log_dens23=pattern3.score_samples(test2)

    log_dens31=pattern1.score_samples(test3)
    log_dens32=pattern2.score_samples(test3)
    log_dens33=pattern3.score_samples(test3)

    for i in range(10):
        if log_dens11[i]>=log_dens12[i] and log_dens11[i]>=log_dens13[i]:
            correct=correct+1

    for i in range(10):
        if log_dens22[i]>=log_dens21[i] and log_dens22[i]>=log_dens23[i]:
            correct=correct+1

    for i in range(10):
        if log_dens33[i]>=log_dens31[i] and log_dens33[i]>=log_dens32[i]:
            correct=correct+1
    
print(correct/150)

```

​	KNN算法对每个测试集的样本，计算它与训练集中的每个对象的欧式距离，找到距离最近的k个训练对象，作为测试对象的近邻，找出前K个点中出现频率最高的类别，作为测试数据的分类结果，用五折交叉检验得出分类性能

​	（2）找到距离最近的k个训练对象，作为测试对象的近邻

​	（3）返回前K个点中出现频率最高的类别作为测试数据的预测分类

```python
def cal_probability(test,x1,x2,x3):
    distance1=np.zeros((1,40))
    for i in range(40):
        distance1[0,i]=np.linalg.norm(test-x1[i,:])
            
    distance2=np.zeros((1,40))
    for i in range(40):
        distance2[0,i]=np.linalg.norm(test-x2[i,:])
            
    distance3=np.zeros((1,40))
    for i in range(40):
        distance3[0,i]=np.linalg.norm(test-x3[i,:])

    sortedDistance1=distance1.copy()
    sortedDistance1.sort(axis=1)
        
    sortedDistance2=distance2.copy()
    sortedDistance2.sort(axis=1)

    sortedDistance3=distance3.copy()
    sortedDistance3.sort(axis=1)

    p1=0
    p2=0
    p3=0

    while 1:
        if(p1+p2+p3)>=k:
            break
        if sortedDistance1[0,p1]<=sortedDistance2[0,p2] and sortedDistance1[0,p1]<=sortedDistance3[0,p3]:
            p1=p1+1
            #print("p1++")
            continue
        if sortedDistance2[0,p2]<=sortedDistance1[0,p1] and sortedDistance2[0,p2]<=sortedDistance3[0,p3]:
            p2=p2+1
            #print("p2++")
            continue
        if sortedDistance3[0,p3]<=sortedDistance2[0,p2] and sortedDistance3[0,p3]<=sortedDistance1[0,p1]:
            p3=p3+1
            #print("p3++")
            continue

    prob1=p1/k
    prob2=p2/k
    prob3=p3/k

    return prob1,prob2,prob3
```



​	3.实验结果分析

​	最大似然估计得到的准确率为0.96（比较特征值正负一个方差的区域之间的的概率）和0.98（直接比较概率密度）

​	高斯核函数估计的准确率：h=0.05时为0.96；h=0.1时为0.96；h=0.2时为0.9667；h=0.4时为0.9733；h=0.5时为0.9667；h=1时为0.9467；h=2时为0.9133；

​	KNN估计的准确率：k=2时为0.9667；k=5时为0.9734；k=10时为0.9867；k=15时为0.9667；k=20时为0.9667；k=30时为0.9467

​	分析：

​	（1）高斯核函数在不同h下的分类性能结果不同：根据上文当中对于核函数的理论叙述我们可知：当h较大时，估计的密度函数就越平滑，偏差会较大，所以准确率会明显降低；当h选的较小，估计的密度曲线和样本拟合的较好，但可能很不光滑，也可能存在过拟合现象。

​	（2)KNN算法在不同的k值下的分类性能结果不同：如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，近似误差（对现有训练集的训练误差）会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，容易发生过拟合，估计误差（对测试集的测试误差）会增大；如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，可以减少学习的估计误差，但近似误差会增大，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误。

​	（3）最大似然估计、高斯核函数估计、KNN算法的性能比较：由实验结果可知，在k=10时的KNN算法性能最好，最大似然估计其次，h=0.4时的高斯核函数再次。分类结果可印证以下的算法性质：KNN算法的准确度高，对数据没有假设，但对异常值不敏感；由参数估计获得的分类器在数据符合高斯（或其他特定的概率分布模型）时，性能较好，但数据若不符合高斯模型，则会导致分类性能很差，也就是依赖于数据假设；高斯核函数可以大大减小计算量，输入空间的维数n对核函数矩阵无影响，核函数方法可以有效处理高维输入，同时，核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。

 