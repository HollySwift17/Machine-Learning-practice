# 线性回归实验报告

信息安全 李涵 1711290

### 一、问题描述

​	现有一组波士顿住房的数据，包括当地的犯罪率、平均房间数、教师与学生比例和一氧化二氮的水平等共13类信息，以及房子的价值，现试用这组数据集来训练得到一个线性回归模型，使得能够根据这些房屋信息预测房屋的价值。

### 二、解决方法

​	1.解决思路

​	基于housing数据集构造线性回归模型，对房价进行预测，采用梯度下降或者随机梯度下降来求解θ，由于数据集规模并不是很大，所以采用留一法来划分训练集和测试集，分别对对于每一个样本进行预测和验证，并输出测试集的RMSE（均方根误差）。在训练的过程中，需要尝试不同的学习率和迭代次数，以选定最佳学习率和迭代次数。为避免对训练集过拟合，对构造的分类器加入正则化系数，L1或L2均可。

​	2.线性回归基本理论

​	（1）概念

​	每个训练样本由d各特征值，即 ![x=(x_{1},x_{2},...,x_{d})](https://www.zhihu.com/equation?tex=x%3D%28x_%7B1%7D%2Cx_%7B2%7D%2C...%2Cx_%7Bd%7D%29) ,其中 ![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D) 是 ![x_{}](https://www.zhihu.com/equation?tex=x_%7B%7D) 在第i个属性上的取值，线性回归模型将各特征值进行线性组合来对测试集的对象进行预测，即

![f(x) =w_{1}x_{1}+w_{2}x_{2}+...+w_{d}x_{d}+b](https://www.zhihu.com/equation?tex=f%28x%29+%3Dw_%7B1%7Dx_%7B1%7D%2Bw_%7B2%7Dx_%7B2%7D%2B...%2Bw_%7Bd%7Dx_%7Bd%7D%2Bb) ，常写成 ![f(x)=w^{T}x+b](https://www.zhihu.com/equation?tex=f%28x%29%3Dw%5E%7BT%7Dx%2Bb) 形式，其中 ![w=(w_{1},w_{2},...,w_{d})](https://www.zhihu.com/equation?tex=w%3D%28w_%7B1%7D%2Cw_%7B2%7D%2C...%2Cw_%7Bd%7D%29) 

​	（2）特点

​	线性模型形式简单，易于建模，是许多功能更为强大的非线性模型的基础。w能够直观体现各属性在预测中的重要性，因此线性模型有很好的可解释性。但线性回归模型对异常值敏感，容易过拟合。

​	（3）最小二乘法

​	试图学得合适的w和b，使得 ![f(x)=w^{T}x+b](https://www.zhihu.com/equation?tex=f%28x%29%3Dw%5E%7BT%7Dx%2Bb) 最接近真实值y。可利用最小二乘法对w和b进行估计。令 ![\tilde{w}=(w;b)](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D%3D%28w%3Bb%29) ，把数据集D表示为一个m*(d+1)大小的矩阵X，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1。则有

​	![\tilde{w^{*}}=arg](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%5E%7B%2A%7D%7D%3Darg) ![min(y-X\tilde{w})^{T}(y-X\tilde{w})](https://www.zhihu.com/equation?tex=min%28y-X%5Ctilde%7Bw%7D%29%5E%7BT%7D%28y-X%5Ctilde%7Bw%7D%29) 

​	令 ![E_{\tilde{w}}=(y-X\tilde{w})^{T}(y-X\tilde{w})](https://www.zhihu.com/equation?tex=E_%7B%5Ctilde%7Bw%7D%7D%3D%28y-X%5Ctilde%7Bw%7D%29%5E%7BT%7D%28y-X%5Ctilde%7Bw%7D%29) ，对 ![\tilde{w}](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) 求导得到

​	![E_{\tilde{w}}=(y-X\tilde{w})^{T}(y-X\tilde{w})](https://www.zhihu.com/equation?tex=E_%7B%5Ctilde%7Bw%7D%7D%3D%28y-X%5Ctilde%7Bw%7D%29%5E%7BT%7D%28y-X%5Ctilde%7Bw%7D%29) 

​	![=(y^{T}-\tilde{w}^{T}x^{T})(y-X\tilde{w})](https://www.zhihu.com/equation?tex=%3D%28y%5E%7BT%7D-%5Ctilde%7Bw%7D%5E%7BT%7Dx%5E%7BT%7D%29%28y-X%5Ctilde%7Bw%7D%29) 

​	![=y^{T}y-y^{T}x\tilde{w}-\tilde{w}^{T}x^{T}y+\tilde{w}x^{T}x\tilde{w}](https://www.zhihu.com/equation?tex=%3Dy%5E%7BT%7Dy-y%5E%7BT%7Dx%5Ctilde%7Bw%7D-%5Ctilde%7Bw%7D%5E%7BT%7Dx%5E%7BT%7Dy%2B%5Ctilde%7Bw%7Dx%5E%7BT%7Dx%5Ctilde%7Bw%7D) 

​	对w求导得

​	![\frac{\alpha E}{\alpha w} = 0-(y^{T}x)^{T}-x^{T}y+2x^{T}x\tilde{w}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha+E%7D%7B%5Calpha+w%7D+%3D+0-%28y%5E%7BT%7Dx%29%5E%7BT%7D-x%5E%7BT%7Dy%2B2x%5E%7BT%7Dx%5Ctilde%7Bw%7D) 

​	![=-x^{T}y-x^{T}y+2x^{T}x\tilde{w}](https://www.zhihu.com/equation?tex=%3D-x%5E%7BT%7Dy-x%5E%7BT%7Dy%2B2x%5E%7BT%7Dx%5Ctilde%7Bw%7D) 

​	![=2x^{T}x\tilde{w}-2x^{T}y](https://www.zhihu.com/equation?tex=%3D2x%5E%7BT%7Dx%5Ctilde%7Bw%7D-2x%5E%7BT%7Dy) 

​	![=2x^{T}(x\tilde{w}-y)](https://www.zhihu.com/equation?tex=%3D2x%5E%7BT%7D%28x%5Ctilde%7Bw%7D-y%29) 

​	令上式为0可得 ![\tilde{w}](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D) 最优解解，当 ![x^{T}x](https://www.zhihu.com/equation?tex=x%5E%7BT%7Dx) 为满秩矩阵或正定矩阵时，令上式为0，得

![\tilde{w^{*}}=(x^{T}x)^{-1}x^{T}y](https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%5E%7B%2A%7D%7D%3D%28x%5E%7BT%7Dx%29%5E%7B-1%7Dx%5E%7BT%7Dy) 

​	其中 ![(x^{T}x)^{-1}](https://www.zhihu.com/equation?tex=%28x%5E%7BT%7Dx%29%5E%7B-1%7D) 是矩阵 ![(x^{T}x)](https://www.zhihu.com/equation?tex=%28x%5E%7BT%7Dx%29) 的逆矩阵。令 ![\tilde{x_{i}}=(x_{i};1)](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx_%7Bi%7D%7D%3D%28x_%7Bi%7D%3B1%29) ，则最终学得的多元线性模型为

![f(\tilde{x}_{i})=\tilde{x_{i}}^{T}(X^{T}X)X^{T}y](https://www.zhihu.com/equation?tex=f%28%5Ctilde%7Bx%7D_%7Bi%7D%29%3D%5Ctilde%7Bx_%7Bi%7D%7D%5E%7BT%7D%28X%5E%7BT%7DX%29X%5E%7BT%7Dy) 

​	（3）梯度下降（批量梯度下降）

​	梯度下降又叫最速下降法，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数

​	已知cost function 代价函数

<img src="https://upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/451" style="zoom:50%" />

​	以二元线性回归为例，对于两个参数分别求偏导：

<img src="https://upload-images.jianshu.io/upload_images/1234352-bfd1c5136eaaa552.png?imageMogr2/auto-orient/" style="zoom:50%" />

​	以合适的学习率α对Θ进行迭代：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1553328180018.png" style="zoom:50%" />

​	一定的迭代次数后得到Θ的合适值

​	（4）随机梯度下降（批量梯度下降）

​	随机梯度下降是在计算下降最快的方向时，随机选一个数据进行计算，而不是扫描全部训练数据集，这样就加快了迭代速度。随机梯度下降并不是沿着J(θ)下降最快的方向收敛，而是震荡的方式趋向极小点。

​	（5）线性回归的正则化

​	为了防止线性回归模型过拟合，在建立线性模型时常常加入正则化项，有L1正则化和L2正则化两种。

​	线性回归的L1正则化通常称为Lasso回归，它和一般线性回归的区别是在损失函数上增加了一个L1正则化的项，由一个常数系数α来调节损失函数的均方差项和正则化项的权重，具体Lasso回归的损失函数表示式为：![f(\tilde{w})=(X\tilde{w}-Y)^{T}(X\tilde{w}-Y)+\alpha||\tilde{w}||_{1}](https://www.zhihu.com/equation?tex=f%28%5Ctilde%7Bw%7D%29%3D%28X%5Ctilde%7Bw%7D-Y%29%5E%7BT%7D%28X%5Ctilde%7Bw%7D-Y%29%2B%5Calpha%7C%7C%5Ctilde%7Bw%7D%7C%7C_%7B1%7D) ，![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 为常数系数，需要进行调优。 ![||\tilde{w}||_{1}](https://www.zhihu.com/equation?tex=%7C%7C%5Ctilde%7Bw%7D%7C%7C_%7B1%7D) 为L1范数。Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0。增强模型的泛化能力。

​	线性回归的L2正则化通常称为Ridge回归，它和一般线性回归的区别是在损失函数上增加了一个L2正则化的项，具体Ridge回归的损失函数表示为：![f(\tilde{w})=(X\tilde{w}-Y)^{T}(X\tilde{w}-Y)+\frac{1}{2}\alpha||\tilde{w}||_{_{2}^{2}}](https://www.zhihu.com/equation?tex=f%28%5Ctilde%7Bw%7D%29%3D%28X%5Ctilde%7Bw%7D-Y%29%5E%7BT%7D%28X%5Ctilde%7Bw%7D-Y%29%2B%5Cfrac%7B1%7D%7B2%7D%5Calpha%7C%7C%5Ctilde%7Bw%7D%7C%7C_%7B_%7B2%7D%5E%7B2%7D%7D) ，![\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 为常数系数，需要进行调优。 ![||\tilde{w}||_{_{2}^{2}}](https://www.zhihu.com/equation?tex=%7C%7C%5Ctilde%7Bw%7D%7C%7C_%7B_%7B2%7D%5E%7B2%7D%7D) 为L2范数（向量各元素的平方和然后求平方根 ）。

​	Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。

​	Ridge回归的梯度下降：<img src="D:\Study\5\机器学习\1711290+李涵+第2次作业\8HWL{S~IBCT45K@SLBF0TU3.png" style="zoom:50%" />

​					或：<img src="D:\Study\5\机器学习\1711290+李涵+第2次作业\(F)U@9(81FA(3XD4AC_ZBCF.png" style="zoom:50%" />

​	3.算法流程（梯度下降）

​	（1）确定合适的学习率和迭代次数

​	（2）给定一个初始theta（全0、全1或者随机）

​	（3）每一次迭代当中计算现有的theta对于y的预测，再计算总的代价

​	（4）用计算出的代价来更新theta，直至迭代结束

​	（5）利用迭代求出的theta做出预测值，并与真实值比较，求出RMSE



### 三、实验分析

​	1.实验数据

​	波士顿住房的数据，共包含506个对象，每个对象都有14个特征值：

- CRIM：犯罪率，按城镇划分的人均犯罪率
- ZN：住宅用地面积25000平方英尺以上的比例
- INDUS：每个城镇非零售商业用地的比例
- CHAS：是否沿河，如果沿河则为1，否则为0
- NOX：一氧化氮浓度(千万分之一)
- RM：每个住宅的平均房间数
- AGE：1940年以前建造的自住单位的比例
- DIS：到五个波士顿就业中心的加权距离
- RAD：辐射状公路可达指数
- TAX：每10,000美元的全价值财产税率
- PTRATIO：按城镇划分的教师与学生比例
- B：1000(Bk - 0.63)^2 （Bk是城镇黑人比例）
- LSTAT：社会地位低的人口
- MEDV：房屋价值（自住房屋的价值中位数，以1000美元为单位）

​	2.实验设计

​	导入数据，并进行基本处理：

```python
import numpy as np
import pandas as pd
import os
import copy
import math

basepath=os.path.abspath(os.path.dirname(__file__))
path=basepath+'/housing.txt'

a=np.loadtxt(path)
A=np.mat(a)

row=A.shape[0]
col=A.shape[1]

rate=0.05#学习率
average=[]#用来存放各特征值的平均值
print("rate")
print(rate)

trainingTimes=100000#迭代次数
print("trainingTimes") 
print(trainingTimes)

RMSE=0 #均方根误差

```

​	计算各特征值的平均值，进行特征缩放：

```python
def adjust(X):
    #特征缩放
    temp=X
    average1=np.sum(temp)/row
    X=temp/average1
    average.append(average1)
    return X

for i in range(col):
    A[:,i]=adjust(A[:,i])

```

​	 批量梯度下降函数：

```python
def bgd(x,y,init_theta,times,rate):
    
    theta = init_theta
    m = x.shape[0]
    for i in range(times):
        predict = x.dot(theta) 
        grad = x.T.dot((predict - y)) / m * rate
        theta -= grad 
    print(theta)
    return theta

```

​	随机梯度下降函数：

```python
def sgd(x, y, init_theta, times, rate):
    theta=init_theta
    for i in range(times):
        j=random.randint(0,row-2)
        temp_x=x[j,:]
        temp_y=y[j,:]
        predict = temp_x.dot(theta) 
        grad = temp_x.T.dot((predict - temp_y)) * rate
        theta -= grad 

    print(theta)
    return theta
```

​	用留一法进行验证，并计算RMSE：

```python
for testIndex in range(row):
    #留一法,对每一行数据进行训练验证
    theta=np.zeros((col,1))
    X=copy.copy(A[:,:])
    for i in range(row):
        X[i,col-1]=1

    Y=copy.copy(A[:,col-1])

    testX=X[testIndex,:]
    testY=Y[testIndex,:]
    X=np.delete(X, testIndex, 0)
    Y=np.delete(Y, testIndex, 0)

    theta=bgd(X,Y,theta.copy(),trainingTimes,rate)

    pred=testX*theta
    print("testY")
    print(testY*average[13])
    print("pred")
    print(pred*average[13])

    RMSE=RMSE+pow((pred[0,0]*average[13]-testY[0,0]*average[13]),2)


RMSE=math.sqrt(RMSE/row)
print("RMSE")
print(RMSE)
```

​	加入正则化系数L2：

```python
def ridge(x,y,init_theta,times,crete,L2):
    theta = init_theta.copy() 
    for j in range(times):
        pred = np.dot(x,theta)
        err = pred-y        
        for i in range(len(theta)): 
            theta[i] = (1-2*rate*L2)*theta[i] - 2*rate*np.dot(err.T,x[:,i])

    print(theta)
    return theta
```



​	3.实验结果分析

​	（1）批量梯度下降：

​	rate=0.02；trainingTimes=100000；RMSE=4.8711178250649585

​	rate=0.02；trainingTimes=200000；RMSE=4.870917257585139

​	rate=0.05；trainingTimes=100000；RMSE=4.87091546907147

​	rate=0.05；trainingTimes=200000；RMSE=4.870908130395694

​	rate=0.1；trainingTimes=100000；RMSE=4.870908130379259

​	rate=0.1；trainingTimes=200000；RMSE=4.870908079555487

​	（2）随机梯度下降：

​	rate=0.01；trainingTimes=500000；RMSE=5.018202437347777

​	rate=0.02；trainingTimes=500000；RMSE=4.870915251596181

​	rate=0.05；trainingTimes=500000；RMSE=4.870907457182139

​	（3）梯度下降和随机梯度下降的比较：

​	实验结果：

​	随机梯度下降的单次迭代时间比梯度下降短，但迭代次数较多，收敛情况基本相同。

​	分析：

​	梯度下降：对于θ的更新，所有的样本都有贡献，也就是参与调整θ ，其计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快

​	随机梯度下降：用样本中的一个例子来近似所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中

​	随机梯度下降算法每次只随机选择一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)

​	不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。 

​	由于波动，因此会使得迭代次数（学习次数）增多，即收敛速度变慢。不过最终其会和全量梯度下降算法一样，具有相同的收敛性，即凸函数收敛于全局极值点，非凸损失函数收敛于局部极值点。

​	（4）梯度下降和Ridge回归的比较：

​	在L2=0.01的条件下得到的RMSE=4.870639746900681，比普通的梯度下降的RMSE略小

​	Ridge回归对系数的大小设置了惩罚项，实质上是一种改良的最小二乘法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，具有较高的准确性、健壮性以及稳定性，但求解速度慢。Ridge回归在不抛弃任何一个变量的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但这会使得模型解释性变差。





​	











